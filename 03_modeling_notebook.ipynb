{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# custom functions for this project\n",
    "from functions import *\n",
    "\n",
    "# dataframe libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# graphing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('ticks')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# text processing\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob as tb\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "import pronouncing\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# miscellany\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# reload functions/libraries when edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# increase column width of dataframe\n",
    "pd.set_option('max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load\n",
    "with gzip.open('data/poetry_umbrella_genres_df_edit.pkl', 'rb') as hello:\n",
    "    df = pickle.load(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['poet_url', 'poem_url', 'poet', 'title', 'poem_lines', 'poem_string',\n",
       "       'genre', 'clean_lines', 'num_lines', 'num_words', 'avg_len_line',\n",
       "       'sentiment_polarity_score', 'sentiment_polarity',\n",
       "       'sentiment_subjectivity_score', 'num_end_rhymes', 'end_rhyme_ratio',\n",
       "       'end_rhyme', 'num_syllables', 'avg_syllables_word', 'lines_titled',\n",
       "       'string_titled', 'string_cleaned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modern          0.292276\n",
       "metropolitan    0.249543\n",
       "pre_1900        0.237888\n",
       "avant_garde     0.220293\n",
       "Name: genre, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a features dataframe\n",
    "X = df[['num_lines', 'avg_len_line', 'sentiment_polarity_score', 'sentiment_subjectivity_score', 'num_end_rhymes', \n",
    "        'end_rhyme', 'avg_syllables_word', 'string_titled', 'string_cleaned']]\n",
    "# assign a target variable\n",
    "y = df['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3282, 9) (3282,)\n",
      "(1094, 9) (1094,)\n"
     ]
    }
   ],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model -- tf-idf vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit to training data's string_cleaned column and transform train and test sets\n",
    "X_train_vec = vectorizer.fit_transform(X_train.string_cleaned)\n",
    "X_test_vec = vectorizer.transform(X_test.string_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the naive bayes classifier\n",
    "bnb_baseline_vec = BernoulliNB()\n",
    "\n",
    "# fit it to our training set\n",
    "bnb_baseline_vec.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.6066422912858014\n",
      "F1 score: 0.6087765944613979\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.4670932358318099\n",
      "F1 score: 0.4253300995343577\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.61      0.08      0.15       241\n",
      " avant_garde       0.66      0.34      0.45       273\n",
      "      modern       0.37      0.92      0.53       320\n",
      "    pre_1900       0.77      0.41      0.53       260\n",
      "\n",
      "    accuracy                           0.47      1094\n",
      "   macro avg       0.60      0.44      0.41      1094\n",
      "weighted avg       0.59      0.47      0.43      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[ 20  33 179   9]\n",
      " [ 12  92 160   9]\n",
      " [  1  12 293  14]\n",
      " [  0   3 151 106]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict the new document from the testing dataset\n",
    "y_train_preds_vec = bnb_baseline_vec.predict(X_train_vec)\n",
    "y_test_preds_vec = bnb_baseline_vec.predict(X_test_vec)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "bnb_baseline_vec_acc_train = accuracy_score(y_train, y_train_preds_vec)\n",
    "bnb_baseline_vec_f1_train = f1_score(y_train, y_train_preds_vec, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {bnb_baseline_vec_acc_train}')\n",
    "print(f'F1 score: {bnb_baseline_vec_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "bnb_baseline_vec_acc_test = accuracy_score(y_test, y_test_preds_vec)\n",
    "bnb_baseline_vec_f1_test = f1_score(y_test, y_test_preds_vec, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {bnb_baseline_vec_acc_test}')\n",
    "print(f'F1 score: {bnb_baseline_vec_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report\n",
    "print(classification_report(y_test, y_test_preds_vec, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not too bad for a baseline. Certainly better than just predicting 'modern', which would give you a 29% accuracy.\n",
    "## Baseline model -- vectors + numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tf-idf vectors into a dataframe so we can attach numerical data\n",
    "X_train_vec_df = pd.DataFrame.sparse.from_spmatrix(X_train_vec, columns=vectorizer.get_feature_names())\n",
    "X_test_vec_df = pd.DataFrame.sparse.from_spmatrix(X_test_vec, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out our numerical data\n",
    "X_train_nums = X_train.drop(columns=['string_titled', 'string_cleaned'])\n",
    "X_test_nums = X_test.drop(columns=['string_titled', 'string_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# scale our numerical data\n",
    "X_train_scaled = scaler.fit_transform(X_train_nums)\n",
    "X_test_scaled = scaler.transform(X_test_nums)\n",
    "\n",
    "# turn the arrays into dataframes\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_nums.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_nums.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the numerical and vector dataframes for both train and test sets\n",
    "X_train_combo = pd.concat([X_train_scaled_df, X_train_vec_df], axis=1)\n",
    "X_test_combo = pd.concat([X_test_scaled_df, X_test_vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the naive bayes classifier\n",
    "bnb_baseline_combo = BernoulliNB()\n",
    "\n",
    "# fit it to our training set\n",
    "bnb_baseline_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.6130408287629494\n",
      "F1 score: 0.6156874992871177\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.4680073126142596\n",
      "F1 score: 0.4299390646948131\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.60      0.10      0.17       241\n",
      " avant_garde       0.65      0.34      0.44       273\n",
      "      modern       0.37      0.91      0.53       320\n",
      "    pre_1900       0.77      0.41      0.53       260\n",
      "\n",
      "    accuracy                           0.47      1094\n",
      "   macro avg       0.60      0.44      0.42      1094\n",
      "weighted avg       0.59      0.47      0.43      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[ 24  35 175   7]\n",
      " [ 14  92 158   9]\n",
      " [  2  12 290  16]\n",
      " [  0   3 151 106]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict the new document from the testing dataset\n",
    "y_train_preds_combo = bnb_baseline_combo.predict(X_train_combo)\n",
    "y_test_preds_combo = bnb_baseline_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "bnb_baseline_combo_acc_train = accuracy_score(y_train, y_train_preds_combo)\n",
    "bnb_baseline_combo_f1_train = f1_score(y_train, y_train_preds_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {bnb_baseline_combo_acc_train}')\n",
    "print(f'F1 score: {bnb_baseline_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "bnb_baseline_combo_acc_test = accuracy_score(y_test, y_test_preds_combo)\n",
    "bnb_baseline_combo_f1_test = f1_score(y_test, y_test_preds_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {bnb_baseline_combo_acc_test}')\n",
    "print(f'F1 score: {bnb_baseline_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report\n",
    "print(classification_report(y_test, y_test_preds_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_combo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A modest bump. A closer look at the confusion matrices do reveal a better spread across true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree -- tf-idf vectors only\n",
    "#### Let's first take a quick look at cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4224924 , 0.38601824, 0.40853659, 0.39329268, 0.38719512,\n",
       "       0.39939024, 0.41158537, 0.42073171, 0.41158537, 0.42073171])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the naive bayes classifier\n",
    "tree_vec = DecisionTreeClassifier()\n",
    "\n",
    "# fit it to our training set and evaluate\n",
    "cross_val_score(tree_vec, X_train_vec, y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes performs better\n",
    "## Decision Tree -- vectors and numerical data\n",
    "#### Look at cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52887538, 0.50151976, 0.50609756, 0.48170732, 0.50304878,\n",
       "       0.4695122 , 0.47865854, 0.4847561 , 0.50914634, 0.4847561 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the decision classifier\n",
    "tree_combo = DecisionTreeClassifier()\n",
    "\n",
    "# fit it to our training set and evaluate\n",
    "cross_val_score(tree_combo, X_train_combo, y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable improvement so let's fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.31 s, sys: 1.01 s, total: 9.32 s\n",
      "Wall time: 9.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "tree_combo = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# fit to the training set\n",
    "tree_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9993906154783668\n",
      "F1 score: 0.9993903971862134\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.47074954296160876\n",
      "F1 score: 0.46965735814376425\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.48      0.46      0.47       241\n",
      " avant_garde       0.39      0.34      0.37       273\n",
      "      modern       0.42      0.48      0.45       320\n",
      "    pre_1900       0.60      0.60      0.60       260\n",
      "\n",
      "    accuracy                           0.47      1094\n",
      "   macro avg       0.48      0.47      0.47      1094\n",
      "weighted avg       0.47      0.47      0.47      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[111  62  59   9]\n",
      " [ 60  94  89  30]\n",
      " [ 44  59 153  64]\n",
      " [ 14  23  66 157]]\n",
      "CPU times: user 6.14 s, sys: 1.3 s, total: 7.44 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict class for the train and test sets\n",
    "y_train_preds_tree_combo = tree_combo.predict(X_train_combo)\n",
    "y_test_preds_tree_combo = tree_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "tree_combo_acc_train = accuracy_score(y_train, y_train_preds_tree_combo)\n",
    "tree_combo_f1_train = f1_score(y_train, y_train_preds_tree_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {tree_combo_acc_train}')\n",
    "print(f'F1 score: {tree_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "tree_combo_acc_test = accuracy_score(y_test, y_test_preds_tree_combo)\n",
    "tree_combo_f1_test = f1_score(y_test, y_test_preds_tree_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {tree_combo_acc_test}')\n",
    "print(f'F1 score: {tree_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_tree_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_tree_combo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitely better than our Naive Bayes model\n",
    "### Decision Tree - Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_end_rhymes': 0.10813965199932314,\n",
       " 'avg_syllables_word': 0.06413765539808662,\n",
       " 'avg_len_line': 0.05971770164042579,\n",
       " 'num_lines': 0.04620431257137106,\n",
       " 'sentiment_subjectivity_score': 0.018340958636556447,\n",
       " 'sentiment_polarity_score': 0.016911472725791556,\n",
       " 'get': 0.01239971179208679,\n",
       " 'end_rhyme': 0.010232488831902102,\n",
       " 'want': 0.00782704258527523,\n",
       " 'form': 0.00748297133618416}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_features = dict(zip(X_train_combo.columns, tree_combo.feature_importances_))\n",
    "tree_features = dict(sorted(tree_features.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "tree_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest -- tf-idf vectors only\n",
    "#### Let's first take a quick look at cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4224924 , 0.38601824, 0.40853659, 0.39329268, 0.38719512,\n",
       "       0.39939024, 0.41158537, 0.42073171, 0.41158537, 0.42073171])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the naive bayes classifier\n",
    "forest_vec = RandomForestClassifier()\n",
    "\n",
    "# fit it to our training set and evaluate\n",
    "cross_val_score(forest_vec, X_train_vec, y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest -- vectors and numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63221884, 0.61094225, 0.6402439 , 0.60670732, 0.5945122 ,\n",
       "       0.5945122 , 0.5945122 , 0.59146341, 0.58841463, 0.6097561 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the decision classifier\n",
    "forest_combo = RandomForestClassifier()\n",
    "\n",
    "# fit it to our training set and evaluate\n",
    "cross_val_score(forest_combo, X_train_combo, y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 1.02 s, total: 18.9 s\n",
      "Wall time: 18.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "forest_combo = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# fit to the training set\n",
    "forest_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9993906154783668\n",
      "F1 score: 0.9993903971862134\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.6206581352833638\n",
      "F1 score: 0.6169936909836253\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.66      0.50      0.57       241\n",
      " avant_garde       0.62      0.50      0.55       273\n",
      "      modern       0.54      0.68      0.60       320\n",
      "    pre_1900       0.72      0.79      0.75       260\n",
      "\n",
      "    accuracy                           0.62      1094\n",
      "   macro avg       0.63      0.62      0.62      1094\n",
      "weighted avg       0.63      0.62      0.62      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[120  60  48  13]\n",
      " [ 30 137  89  17]\n",
      " [ 30  23 217  50]\n",
      " [  3   1  51 205]]\n",
      "CPU times: user 6.32 s, sys: 1.29 s, total: 7.61 s\n",
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict class for the train and test sets\n",
    "y_train_preds_forest_combo = forest_combo.predict(X_train_combo)\n",
    "y_test_preds_forest_combo = forest_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "forest_combo_acc_train = accuracy_score(y_train, y_train_preds_forest_combo)\n",
    "forest_combo_f1_train = f1_score(y_train, y_train_preds_forest_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {forest_combo_acc_train}')\n",
    "print(f'F1 score: {forest_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "forest_combo_acc_test = accuracy_score(y_test, y_test_preds_forest_combo)\n",
    "forest_combo_f1_test = f1_score(y_test, y_test_preds_forest_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {forest_combo_acc_test}')\n",
    "print(f'F1 score: {forest_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_forest_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_forest_combo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_len_line': 0.017643350734591664,\n",
       " 'num_end_rhymes': 0.016250908575151858,\n",
       " 'end_rhyme': 0.014684254285136953,\n",
       " 'avg_syllables_word': 0.011260227256186814,\n",
       " 'num_lines': 0.00809932600319891,\n",
       " 'sentiment_polarity_score': 0.007727050910427052,\n",
       " 'sentiment_subjectivity_score': 0.006358502933598742,\n",
       " 'get': 0.0030625540008904274,\n",
       " 'day': 0.002957372822259227,\n",
       " 'heart': 0.00294727064951319}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_features = dict(zip(X_train_combo.columns, forest_combo.feature_importances_))\n",
    "forest_features = dict(sorted(forest_features.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "forest_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model - tf-idf vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.84 s, sys: 16.7 ms, total: 7.85 s\n",
      "Wall time: 7.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "svm_vec = SVC(kernel='linear')\n",
    "\n",
    "# fit to the training set\n",
    "svm_vec.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9686166971358927\n",
      "F1 score: 0.9685752094288665\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.6261425959780622\n",
      "F1 score: 0.6247558000810909\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.54      0.44      0.48       241\n",
      " avant_garde       0.58      0.61      0.60       273\n",
      "      modern       0.57      0.67      0.62       320\n",
      "    pre_1900       0.82      0.77      0.80       260\n",
      "\n",
      "    accuracy                           0.63      1094\n",
      "   macro avg       0.63      0.62      0.62      1094\n",
      "weighted avg       0.63      0.63      0.62      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[105  68  62   6]\n",
      " [ 41 167  55  10]\n",
      " [ 38  42 213  27]\n",
      " [  9   9  42 200]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict class for the train and test sets\n",
    "y_train_preds_svm_vec = svm_vec.predict(X_train_vec)\n",
    "y_test_preds_svm_vec = svm_vec.predict(X_test_vec)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "svm_vec_acc_train = accuracy_score(y_train, y_train_preds_svm_vec)\n",
    "svm_vec_f1_train = f1_score(y_train, y_train_preds_svm_vec, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {svm_vec_acc_train}')\n",
    "print(f'F1 score: {svm_vec_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "svm_vec_acc_test = accuracy_score(y_test, y_test_preds_svm_vec)\n",
    "svm_vec_f1_test = f1_score(y_test, y_test_preds_svm_vec, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {svm_vec_acc_test}')\n",
    "print(f'F1 score: {svm_vec_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_svm_vec, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_svm_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quite a bump! very overfit, but let's see if it works any better with our combo dataframes\n",
    "## SVM - vectors + numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 37s, sys: 1.73 s, total: 6min 39s\n",
      "Wall time: 6min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "svm_combo = SVC(kernel='linear')\n",
    "\n",
    "# fit to the training set\n",
    "svm_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9570383912248629\n",
      "F1 score: 0.9569679418427056\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.6828153564899452\n",
      "F1 score: 0.6820809987075851\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.66      0.61      0.63       241\n",
      " avant_garde       0.64      0.63      0.64       273\n",
      "      modern       0.63      0.66      0.65       320\n",
      "    pre_1900       0.80      0.83      0.81       260\n",
      "\n",
      "    accuracy                           0.68      1094\n",
      "   macro avg       0.69      0.68      0.68      1094\n",
      "weighted avg       0.68      0.68      0.68      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[147  53  38   3]\n",
      " [ 37 173  51  12]\n",
      " [ 34  36 212  38]\n",
      " [  4   7  34 215]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict class for the train and test sets\n",
    "y_train_preds_svm_combo = svm_combo.predict(X_train_combo)\n",
    "y_test_preds_svm_combo = svm_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "svm_combo_acc_train = accuracy_score(y_train, y_train_preds_svm_combo)\n",
    "svm_combo_f1_train = f1_score(y_train, y_train_preds_svm_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {svm_combo_acc_train}')\n",
    "print(f'F1 score: {svm_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "svm_combo_acc_test = accuracy_score(y_test, y_test_preds_svm_combo)\n",
    "svm_combo_f1_test = f1_score(y_test, y_test_preds_svm_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {svm_combo_acc_test}')\n",
    "print(f'F1 score: {svm_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_svm_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_svm_combo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still overfit but nearly a 10% increase in our test predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 41s, sys: 3.3 s, total: 8min 45s\n",
      "Wall time: 8min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=2, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "svm_combo2 = SVC(C=2, kernel='linear', class_weight='balanced')\n",
    "\n",
    "# fit to the training set\n",
    "svm_combo2.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9954296160877514\n",
      "F1 score: 0.995428457373375\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.6892138939670932\n",
      "F1 score: 0.6885612715284259\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.67      0.63      0.65       241\n",
      " avant_garde       0.63      0.66      0.65       273\n",
      "      modern       0.65      0.64      0.64       320\n",
      "    pre_1900       0.81      0.84      0.83       260\n",
      "\n",
      "    accuracy                           0.69      1094\n",
      "   macro avg       0.69      0.69      0.69      1094\n",
      "weighted avg       0.69      0.69      0.69      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[152  50  35   4]\n",
      " [ 33 180  49  11]\n",
      " [ 36  45 204  35]\n",
      " [  7   9  26 218]]\n",
      "CPU times: user 8min 1s, sys: 2.92 s, total: 8min 4s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# predict class for the train and test sets\n",
    "y_train_preds_svm_combo2 = svm_combo2.predict(X_train_combo)\n",
    "y_test_preds_svm_combo2 = svm_combo2.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "svm_combo_acc_train2 = accuracy_score(y_train, y_train_preds_svm_combo2)\n",
    "svm_combo_f1_train2 = f1_score(y_train, y_train_preds_svm_combo2, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {svm_combo_acc_train2}')\n",
    "print(f'F1 score: {svm_combo_f1_train2}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "svm_combo_acc_test2 = accuracy_score(y_test, y_test_preds_svm_combo2)\n",
    "svm_combo_f1_test2 = f1_score(y_test, y_test_preds_svm_combo2, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {svm_combo_acc_test2}')\n",
    "print(f'F1 score: {svm_combo_f1_test2}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_svm_combo2, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_svm_combo2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-9290159fd349>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-9290159fd349>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    please break my code\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "please break my code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Net -- tf-idf vectors only\n",
    "#### First we need to convert our target classes into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and transform the target train set\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "\n",
    "# transfomr the target test set\n",
    "y_test_le = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code--including vocab_size, embedding_dim, and num_epochs--based on work by from Susan Li (https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # add an Embedding layer expecting input vocab of size 5000,\n",
    "    # and output embedding dimension of size 64 we set at the top\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Bidirectional(LSTM(embedding_dim)),\n",
    "    Dense(embedding_dim, activation='relu'),\n",
    "    # add a Dense layer with 6 units and softmax activation, since we have multiple classes\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 5\n",
    "history = model.fit(X_train_vec, y_train_le, epochs=num_epochs,\n",
    "                    validation_data=(X_test_vec, y_test_le), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
