{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for this project\n",
    "from functions import *\n",
    "\n",
    "# dataframe libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# graphing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('ticks')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# text processing\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob as tb\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "import pronouncing\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# miscellany\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# reload functions/libraries when edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# increase column width of dataframe\n",
    "pd.set_option('max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load\n",
    "with gzip.open('data/poetry_umbrella_genres_df_edit.pkl', 'rb') as hello:\n",
    "    df = pickle.load(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['poet_url', 'poem_url', 'poet', 'title', 'poem_lines', 'poem_string',\n",
       "       'genre', 'clean_lines', 'num_lines', 'num_words', 'avg_len_line',\n",
       "       'sentiment_polarity_score', 'sentiment_polarity',\n",
       "       'sentiment_subjectivity_score', 'num_end_rhymes', 'end_rhyme_ratio',\n",
       "       'end_rhyme', 'num_syllables', 'avg_syllables_word', 'lines_titled',\n",
       "       'string_titled', 'string_cleaned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modern          0.292276\n",
       "metropolitan    0.249543\n",
       "pre_1900        0.237888\n",
       "avant_garde     0.220293\n",
       "Name: genre, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a features dataframe\n",
    "X = df[['num_lines', 'avg_len_line', 'sentiment_polarity_score', 'sentiment_subjectivity_score', 'num_end_rhymes', \n",
    "        'end_rhyme', 'avg_syllables_word', 'string_titled', 'string_cleaned']]\n",
    "# assign a target variable\n",
    "y = df['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3282, 9) (3282,)\n",
      "(1094, 9) (1094,)\n"
     ]
    }
   ],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model -- just vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit to training data's string_cleaned column and transform train and test sets\n",
    "X_train_vec = vectorizer.fit_transform(X_train.string_cleaned)\n",
    "X_test_vec = vectorizer.transform(X_test.string_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the naive bayes classifier\n",
    "bnb_baseline_vec = BernoulliNB()\n",
    "\n",
    "# fit it to our training set\n",
    "bnb_baseline_vec.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.6066422912858014\n",
      "F1 score: 0.6087765944613979\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.4670932358318099\n",
      "F1 score: 0.4253300995343577\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.61      0.08      0.15       241\n",
      " avant_garde       0.66      0.34      0.45       273\n",
      "      modern       0.37      0.92      0.53       320\n",
      "    pre_1900       0.77      0.41      0.53       260\n",
      "\n",
      "    accuracy                           0.47      1094\n",
      "   macro avg       0.60      0.44      0.41      1094\n",
      "weighted avg       0.59      0.47      0.43      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[ 20  33 179   9]\n",
      " [ 12  92 160   9]\n",
      " [  1  12 293  14]\n",
      " [  0   3 151 106]]\n"
     ]
    }
   ],
   "source": [
    "# predict the new document from the testing dataset\n",
    "y_train_preds_vec = bnb_baseline_vec.predict(X_train_vec)\n",
    "y_test_preds_vec = bnb_baseline_vec.predict(X_test_vec)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "bnb_baseline_vec_acc_train = accuracy_score(y_train, y_train_preds_vec)\n",
    "bnb_baseline_vec_f1_train = f1_score(y_train, y_train_preds_vec, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {bnb_baseline_vec_acc_train}')\n",
    "print(f'F1 score: {bnb_baseline_vec_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "bnb_baseline_vec_acc_test = accuracy_score(y_test, y_test_preds_vec)\n",
    "bnb_baseline_vec_f1_test = f1_score(y_test, y_test_preds_vec, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {bnb_baseline_vec_acc_test}')\n",
    "print(f'F1 score: {bnb_baseline_vec_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report\n",
    "print(classification_report(y_test, y_preds, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not too bad for a baseline. Certainly better than just predicting 'modern', which would give you a 29% accuracy.\n",
    "## Baseline model -- vectors + numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tf-idf vectors into a dataframe so we can attach numerical data\n",
    "X_train_vec_df = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_test_vec_df = pd.DataFrame.sparse.from_spmatrix(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out our numerical data\n",
    "X_train_nums = X_train.drop(columns=['string_titled', 'string_cleaned'])\n",
    "X_test_nums = X_test.drop(columns=['string_titled', 'string_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale our numerical data\n",
    "X_train_scaled = scaler.fit_transform(X_train_nums)\n",
    "X_test_scaled = scaler.transform(X_test_nums)\n",
    "\n",
    "# turn the arrays into dataframes\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the numerical and vector dataframes for both train and test sets\n",
    "X_train_combo = pd.concat([X_train_scaled_df, X_train_vec_df], axis=1)\n",
    "X_test_combo = pd.concat([X_test_scaled_df, X_test_vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the naive bayes classifier\n",
    "bnb_baseline_combo = BernoulliNB()\n",
    "\n",
    "# fit it to our training set\n",
    "bnb_baseline_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.6118220597196831\n",
      "F1 score: 0.6144550375279676\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.47623400365630714\n",
      "F1 score: 0.4386438654941913\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.73      0.10      0.18       241\n",
      " avant_garde       0.67      0.36      0.47       273\n",
      "      modern       0.38      0.91      0.53       320\n",
      "    pre_1900       0.75      0.41      0.53       260\n",
      "\n",
      "    accuracy                           0.48      1094\n",
      "   macro avg       0.63      0.45      0.43      1094\n",
      "weighted avg       0.62      0.48      0.44      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[ 24  34 174   9]\n",
      " [  8  99 157   9]\n",
      " [  1  11 291  17]\n",
      " [  0   3 150 107]]\n"
     ]
    }
   ],
   "source": [
    "# predict the new document from the testing dataset\n",
    "y_train_preds_combo = bnb_baseline_combo.predict(X_train_combo)\n",
    "y_test_preds_combo = bnb_baseline_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "bnb_baseline_combo_acc_train = accuracy_score(y_train, y_train_preds_combo)\n",
    "bnb_baseline_combo_f1_train = f1_score(y_train, y_train_preds_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {bnb_baseline_combo_acc_train}')\n",
    "print(f'F1 score: {bnb_baseline_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "bnb_baseline_combo_acc_test = accuracy_score(y_test, y_test_preds_combo)\n",
    "bnb_baseline_combo_f1_test = f1_score(y_test, y_test_preds_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {bnb_baseline_combo_acc_test}')\n",
    "print(f'F1 score: {bnb_baseline_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report\n",
    "print(classification_report(y_test, y_test_preds_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_combo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A modest bump. A closer look at the confusion matrices do reveal a better spread across true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model - just vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 s, sys: 41.7 ms, total: 8.58 s\n",
      "Wall time: 8.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "svm_vec = SVC(kernel='linear')\n",
    "\n",
    "# fit to the training set\n",
    "svm_vec.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9686166971358927\n",
      "F1 score: 0.9685752094288665\n",
      "\n",
      "-----TEST-----\n",
      "Accuracy: 0.6261425959780622\n",
      "F1 score: 0.6247558000810909\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.54      0.44      0.48       241\n",
      " avant_garde       0.58      0.61      0.60       273\n",
      "      modern       0.57      0.67      0.62       320\n",
      "    pre_1900       0.82      0.77      0.80       260\n",
      "\n",
      "    accuracy                           0.63      1094\n",
      "   macro avg       0.63      0.62      0.62      1094\n",
      "weighted avg       0.63      0.63      0.62      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[105  68  62   6]\n",
      " [ 41 167  55  10]\n",
      " [ 38  42 213  27]\n",
      " [  9   9  42 200]]\n"
     ]
    }
   ],
   "source": [
    "# predict class for the train and test sets\n",
    "y_train_preds_svm_vec = svm_vec.predict(X_train_vec)\n",
    "y_test_preds_svm_vec = svm_vec.predict(X_test_vec)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "svm_vec_acc_train = accuracy_score(y_train, y_train_preds_svm_vec)\n",
    "svm_vec_f1_train = f1_score(y_train, y_train_preds_svm_vec, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {svm_vec_acc_train}')\n",
    "print(f'F1 score: {svm_vec_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "svm_vec_acc_test = accuracy_score(y_test, y_test_preds_svm_vec)\n",
    "svm_vec_f1_test = f1_score(y_test, y_test_preds_svm_vec, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {svm_vec_acc_test}')\n",
    "print(f'F1 score: {svm_vec_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_svm_vec, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_svm_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quite a bump! very overfit, but let's see if it works any better with our combo dataframes\n",
    "## SVM - vectors + numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 1s, sys: 2.28 s, total: 7min 3s\n",
      "Wall time: 7min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate the model\n",
    "svm_combo = SVC(kernel='linear')\n",
    "\n",
    "# fit to the training set\n",
    "svm_combo.fit(X_train_combo, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----TRAIN-----\n",
      "Accuracy: 0.9552102376599635\n",
      "F1 score: 0.9551191068947608\n",
      "-----TEST-----\n",
      "Accuracy: 0.6809872029250457\n",
      "F1 score: 0.6801702436944882\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "metropolitan       0.66      0.61      0.63       241\n",
      " avant_garde       0.64      0.63      0.64       273\n",
      "      modern       0.63      0.66      0.65       320\n",
      "    pre_1900       0.80      0.82      0.81       260\n",
      "\n",
      "    accuracy                           0.68      1094\n",
      "   macro avg       0.68      0.68      0.68      1094\n",
      "weighted avg       0.68      0.68      0.68      1094\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[147  51  39   4]\n",
      " [ 39 172  49  13]\n",
      " [ 34  36 212  38]\n",
      " [  4   8  34 214]]\n"
     ]
    }
   ],
   "source": [
    "# predict class for the train and test sets\n",
    "y_train_preds_svm_combo = svm_combo.predict(X_train_combo)\n",
    "y_test_preds_svm_combo = svm_combo.predict(X_test_combo)\n",
    "\n",
    "# print out accuracy and f1 scores for train set\n",
    "svm_combo_acc_train = accuracy_score(y_train, y_train_preds_svm_combo)\n",
    "svm_combo_f1_train = f1_score(y_train, y_train_preds_svm_combo, average='weighted')\n",
    "\n",
    "print('-----TRAIN-----')\n",
    "print(f'Accuracy: {svm_combo_acc_train}')\n",
    "print(f'F1 score: {svm_combo_f1_train}')\n",
    "\n",
    "# print out accuracy and f1 scores for test set\n",
    "svm_combo_acc_test = accuracy_score(y_test, y_test_preds_svm_combo)\n",
    "svm_combo_f1_test = f1_score(y_test, y_test_preds_svm_combo, average='weighted')\n",
    "\n",
    "print('\\n-----TEST-----')\n",
    "print(f'Accuracy: {svm_combo_acc_test}')\n",
    "print(f'F1 score: {svm_combo_f1_test}')\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out report for test predictions\n",
    "print(classification_report(y_test, y_test_preds_svm_combo, target_names=list(y.unique())))\n",
    "\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# print out confusion matrix\n",
    "print(\"CONFUSION MATRIX:\\n\")\n",
    "print(confusion_matrix(y_test, y_test_preds_svm_combo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still overfit but nearly a 10% increase in our test predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
